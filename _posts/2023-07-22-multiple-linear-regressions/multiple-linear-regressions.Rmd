---
title: "Multiple Linear Regressions"
description: |
  Quick analysis in pine trees
author:
  - name: Gabriel de Freitas Pereira
    url: {}
date: 2023-07-23
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library( tidyverse )
library( skimr )
library( tidymodels )
library( forcats )
library( ggrepel )
options( scipen = 9999 )
```

\  
<div style="text-align: center">  
## Multiple LM by functional approach ##
</div>
\ 

# Quick analysis

\  

\  Well, the idea here is to create multiple linear models in a straight forward analyzes to understand the relative changes between height in different ages and seeds of *Pinus taeda*, commonly known as loblolly pine, which is one of several pines native to the Southeastern United States. In order to do that I am going to apply a few statistical concepts to analyze the p-value and the estimates generated by the model, using the dataset called `Loblolly`, here it is the overview of it:

```{r, echo=FALSE}
skim( Loblolly )
```

```{r, echo=FALSE}
Loblolly %>% 
  ggplot( aes( height, age ) ) +
  geom_point( size = 2.7, alpha = 0.7, color = "darkgreen" ) +
  facet_wrap( ~Seed ) +
  labs( x = "Height (ft)", y = "Age" ) +
  theme_bw( )
```

Observation: **I am aware that in a real world scenario we would need a bigger sample size in order to get reliable linear regression models, but here I am focused on studying the approach instead of checking all the assumptions required by the model**.

\  

# Models 

\  

\  I would like to go through somethings that I am considering here before jump to the modeling. So is important to mention that the p-value is a statistical measure that indicates the probability of obtaining the observed results, or even more extreme results, when the null hypothesis is true. In other words, it is a measure that helps us assess whether the available evidence suggests that an observed difference or effect is statistically significant or just a random result.

\  When we perform multiple statistical comparisons on the same dataset, we increase the probability of obtaining at least one significant result (i.e., a p-value smaller than the chosen significance level) by chance alone, even if there is no true effect present. This phenomenon is known as the "multiple testing problem" or "multiple comparisons problem", which can be solved through p-value adjustment.

\  To illustrate the importance of p-value adjustment, consider an example where you conduct 20 independent hypothesis tests, each with a significance level of 0.05. The probability of obtaining at least one false positive (false significant result) is higher than 1 - (1 - 0.05)^20 â‰ˆ 0.64, which is approximately 64%. This means that if you do not adjust the p-values, you run a relatively high risk of erroneously concluding that there are significant effects when they do not exist.

\  Therefore to address this problem, there are several adjustment techniques. On this analysis I am going to use the Bonferroni correction in which the p-values are multiplied by the number of comparisons. Controlling the overall significance level to reduce the risk of false positives and make multiple comparisons more reliable.

\  Besides that, I am not interested in the Intercepts generated, which represents the estimated age when the height is zero (makes no sense at this case). Only in the slopes, which represents the estimated change in height associated with a one-unit increase in age. These coefficients provide crucial information about the relationship between height and age across different Seeds. Thus, by analyzing the slopes, we can understand how the height of trees responds to changes in age. 

Finally in order to accomplish all that was mentioned this code was written: 

```{r}
tidy_lm <- Loblolly %>%
  nest( data = c( height, age ) ) %>%
  mutate( model = map( data, ~ lm( height ~ age, data = .x ) ) )

slopes <- tidy_lm %>% 
  mutate( coefs = map( model, tidy ) ) %>% 
  unnest( coefs ) %>% 
  filter( term == "age" ) %>% 
  mutate( p.value = p.adjust( p.value ) )

slopes
```
\  

# Results

```{r}
slopes %>% 
  ggplot( aes( estimate, p.value, label = Seed ) ) +
  geom_vline( xintercept = 0, lty = 2, linewidth = 0.9, alpha = 0.7, color = "gray30" ) +
  geom_point( aes( color = Seed ), alpha = 0.8, size = 2.5, show.legend = FALSE ) +
  facet_wrap( ~Seed ) +
  labs( x = "estimates", title = "Increase in height per age" ) +
  theme_bw( )  
```
 
# Conclusion

\  

\  The plot shows the relationship between the increase in height per age (slopes) for different levels of the Seed variable in the Loblolly dataset. Each facet represents a specific level of the Seed variable. The points on the plot represent the slopes for each seed. The dashed gray line at x = 0 represents the reference line where there is no increase or decrease in height with age.

- Everything is in the right side because as expected the height is increasing over the years. **The interesting thing about this analysis is to understand that the further to the right it is, the larger the increase over this time period, enabling a quick overview between seeds.** In summary we can highlight the 321 seed, due the low p-value and the high estimate. 

- The p-value measures the probability of obtaining results as extreme as those observed, assuming that the null hypothesis (no relationship or no change with time) is true. Lower p-values on the y-axis of the plot indicate stronger evidence against the null hypothesis, suggesting more confidence in the existence of real relationships between height and age which at this case is very obvious.

\  In conclusion is important to mention that this approach for using statistical models to estimate changes in many subgroups at once is very useful in different situations and can be applied to get reliable insights across the data.

