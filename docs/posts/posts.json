[
  {
    "path": "posts/2024-10-22-eda-clustering-data/",
    "title": "EDA + Clustering Data",
    "description": "Exploratory data analysis and the application of unsupervised learning techniques (K-means and HDBSCAN)",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2024-10-22",
    "categories": [],
    "contents": "\nIntroduction\nHi there! It’s been a while since my last post, and I’m excited to share an analysis\nI’ve been working on as part of my new journey in pursuing a Master’s degree in Computer Science at the same\nuniversity where I completed my undergraduate studies (UFSCar).\nThe first course I took in this program was focused on Unsupervised and Semi-supervised Machine Learning. It’s been a fascinating experience so far, and I’ve learned some interesting techniques that I’m eager to explore in this post. Specifically, I’ll be using K-means and Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) in order to create clusters and identify patterns in products from Adidas and Nike.\nDataset\nFor this analysis, I used a dataset that contains detailed information on sales and other relevant aspects of Adidas and Nike products. The dataset consists of 3,268 products from both brands, with 12 attributes related to these products. You can access this dataset for free on Kaggle via this link.\nHere it is the overview of it:\n\nTable 1: Data summary\nName\nbrands\nNumber of rows\n3268\nNumber of columns\n10\n_______________________\n\nColumn type frequency:\n\ncharacter\n5\nnumeric\n5\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nProduct.Name\n0\n1\n4\n73\n0\n1531\n0\nProduct.ID\n0\n1\n6\n10\n0\n3179\n0\nBrand\n0\n1\n4\n24\n0\n5\n0\nDescription\n0\n1\n0\n687\n3\n1763\n0\nLast.Visited\n0\n1\n19\n19\n0\n318\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nListing.Price\n0\n1\n6868.02\n4724.66\n0\n4299.0\n5999.0\n8999.0\n29999\n▇▆▂▁▁\nSale.Price\n0\n1\n6134.27\n4293.25\n449\n2999.0\n4799.0\n7995.0\n36500\n▇▂▁▁▁\nDiscount\n0\n1\n26.88\n22.63\n0\n0.0\n40.0\n50.0\n60\n▇▁▁▅▆\nRating\n0\n1\n3.24\n1.43\n0\n2.6\n3.5\n4.4\n5\n▃▁▅▆▇\nReviews\n0\n1\n40.55\n31.54\n0\n10.0\n37.0\n68.0\n223\n▇▆▁▁▁\n\nExploratory Data Analysis\nI started by creating visualizations that focus on the Sale Price and Listing Price distributions. By overlaying these histograms, we aim to identify patterns in pricing strategies, such as differences in pricing ranges, which can provide valuable insights into market behavior.\n\n\n\nFigure 1: Comparative Analysis of Sale Price vs Listing Price for Adidas and Nike\n\n\n\nBefore diving deeper into this, I’d like to review the other variables. This might help on identifying any inconsistencies or elements that may not be useful for the analysis.\n\n\n\nFigure 2: Frequency of Brand Categories\n\n\n\nWith the insight brought by the visualization above this product called Adidas Adidas Originals was reassigned for the correct category called Adidas Originals. This way we are going to remain with only 4 categories instead of 5 given the inconsistency found in the Brand assignment.\n\n\n\nFigure 3: Comparative Analysis of Sale Price vs Listing Price by Brand\n\n\n\nAfter visualizing the information contained in the Figure 3, a couple of interesting questions come to mind:\n- How many products have a Listing Price higher than the Sale Price?\n\n\n\nFigure 4: Comparison of Listing Price vs Sale Price by Brand\n\n\n\n- Why is there a difference between Listing Price and Sale Price? Is it always due to discounts?\nWell no, among the 876 products with an available Listing Price and no explicit discounts, 217 have a Sale Price lower than the Listing Price. This suggests that the difference isn’t always due to planned discounts, leading to a few possible explanations:\nUnrecorded Discounts: Discounts may have been applied but not properly documented in the data.\nDynamic Pricing: Prices may vary due to demand, stock, or customer behavior, without being categorized as discounts.\nSystem or Labeling Errors: Price discrepancies could result from labeling or system errors.\nCompetitive Pricing: Companies may lower prices to stay competitive, even without formal discounts.\nPromotions: Other forms of price reductions might not be labeled as discounts but still lower the sale price.\nMarket Value Changes: Prices might be reduced over time to reflect changes in demand or product value.\nWhich brand segment is most affected by these unrecorded changes?\n\n\nreactable( \n  brands %>%\n  group_by( Brand ) %>%\n  filter( Listing.Price > Sale.Price, Discount == 0 ) %>%\n  count( )\n)\n\n\n\nAs seen above, all products that do not show a discount in the data but had changes in the final sale price are Nike products. In fact, every Nike product with a listing price underwent adjustments. This suggests either that these products do not have the correct listing price, or the discount is not being recorded for these products (the latter is more likely, as no Nike product in the dataset has a documented discount).\n- Are there any products with a Sale Price higher than the Listing Price?\n\n\n\nThis is unexpected, right? The Listing Price should be equal to or higher than the Sale Price, not the other way around. I’ll take a closer look at these Nike products.\n\n\nreactable( \n  brands %>% \n  select( Product.Name, Listing.Price, Sale.Price, Discount ) %>% \n  filter( Sale.Price > Listing.Price ), defaultPageSize = 5\n)\n\n\n\nAn intriguing issue in this table is that these products with higher sale price have a listing price of 0, which suggests a potential error in the data generation process, as it’s unlikely for a product to be listed without a price. To correct this, I will assign the sale price value to these cases, adjusting based on the discount column (if available):\n\n\nbrands_input <- \n  brands %>%\n  mutate( Listing.Price = \n            case_when( \n              Listing.Price == 0 & Discount == 0 ~ Sale.Price,\n              Listing.Price == 0 & Discount > 0 ~ Sale.Price * Discount / 100,\n              TRUE ~ Listing.Price  \n          )\n  )\n\n\nAs I mentioned before, none of the Nike products have a discount available in the dataset. But what about the Adidas products?\n\n\n\nFigure 5: Distribution of Discounts Across Adidas Segments\n\n\n\nThe flow chart from the Figure 5 shows that CORE / NEO segment had the highest percentage of products with discounts over 30%. Ok, so let’s explore ratings description and reviews before building our product clusters.\n\n\n\nFigure 6: Distribution of Ratings Across Segments\n\n\n\nThe difference between the brands’ ratings is notable; however, it is important to point out that they have different numbers of products, with Nike having the fewest representatives in the dataset, which may explain the high standard deviation observed. Additionally, the dataset includes product descriptions, and given that it contains only shoes, it would be interesting to explore the common words used in these descriptions to gain insights into product characteristics or marketing trends.\n\n\n\nFigure 7: Frequency of Words by Brand\n\n\n\nNaturally, the first question that arises from this plot is whether there’s any relationship between the words used and the product ratings. To explore this further, I conducted an analysis and visualized the findings in the form of this eye-catching word cloud:\n\n\n\nFigure 8: Frequency of Words by Brand\n\n\n\n\n\n\nFigure 9: Distribution of Reviews by Brand\n\n\n\nThe word cloud and box plot together highlight the contrasting dynamics between Nike and Adidas in terms of customer engagement and review distribution. In the word cloud, Nike-related terms like “modernised” and “air” are prominent, suggesting that these products dominate discussions and receive significant attention. This is mirrored in the box plot, where Nike shows an irregular review distribution, with numerous outliers indicating that while some Nike products receive a large volume of reviews, others are reviewed much less frequently. In contrast, Adidas displays more balanced word frequencies in the word cloud and a more consistent review distribution across its product lines, as seen in the box plot. This suggests that Adidas products tend to attract steadier, more uniform customer engagement, without the extremes observed in Nike’s reviews.\nBefore moving into modeling, it’s important to note that our data does not exhibit spherical dispersion. As a result, algorithms that form globular clusters, such as K-means, may not be ideal. Nonetheless, I will still test K-means to compare the clusters they form.\n\n\n\nFigure 10: Distribution of Data Based on Sale and Listing Prices\n\n\n\nThe figure above shows the distribution of two key variables: Listing Price and Sale Price. These variables are essential for analyzing pricing strategies and product performance, and they play a crucial role in forming clusters to understand business dynamics in retail and e-commerce.\nBuilding Clusters with K-means\nFirst, the data was scaled to minimize the impact of varying units across different variables. The following variables were initially tested:\nSale Price\nListing Price\nDiscount\nRatings\nNumber of Reviews\nAs a reminder, variables where the Listing Price was 0 were adjusted to match the Sale Price, accounting for any available Discount.\n\n\n\nFigure 11: Elbow Method for Determining the Optimal Number of Clusters on K-means algorithm\n\n\n\n\n\n\nFigure 12: K-means Clusters\n\n\n\nThe elbow method suggests that two clusters are optimal for this dataset. The K-means algorithm was applied to the scaled data, resulting in two distinct clusters. The plot shows the distribution of data points based on Sale Price and Listing Price, with each point colored according to its assigned cluster. While the two clusters exhibit a roughly globular dispersion, the data appears to have more complexity and does not fully adhere to this structure, indicating that K-means may not be the best algorithm to capture the underlying characteristics of the products.\nBuilding Clusters with HDBSCAN\nThe HDBSCAN algorithm identified 10 distinct clusters and 972 points considered noise. Noise points are those that do not fit well into any of the formed clusters. The sizes of these clusters vary significantly, which is typical for density-based algorithms like HDBSCAN.\n\n\n\nFigure 13: HDBSCAN Cluster Plot Showing Consistent Branches\n\n\n\nFor instance, Cluster 4 is the largest, containing 930 objects, indicating a region of high density. In contrast, Cluster 5 has only 12 objects, suggesting a region of much lower density. Interestingly, Cluster 0 represents noise, with 972 points, indicating a significant portion of the data does not clearly belong to any cluster. The remaining clusters capture variations in sale and listing prices, reviews, and ratings.\n\n\n\nFigure 14: HDBSCAN Clusters\n\n\n\nThe maximum sale prices across clusters range from $3,197 in smaller clusters up to\n$36,500 in the noise points. Cluster 4 stands out with its large number of reviews and higher average rating of 3.9, while Cluster 5 has no reviews or ratings, indicating limited customer interaction. Clusters like Cluster 1 and Cluster 2 are smaller in size, with moderate sale prices and no significant discounts.\nTable summarizing the clusters formed by HDBSCAN:\n\n\n\nConclusion\nIn conclusion, HDBSCAN effectively identifies regions of varying density in the dataset, capturing both high and low-density clusters. This clustering offers valuable insights into different pricing strategies and product performance, where larger clusters indicate products with higher customer engagement and more stable pricing, while smaller clusters may indicate niche products or items with less market activity. The noise points, accounting for a large proportion, might represent outliers or products that don’t fit well into the overall market patterns.\nIn a real-world scenario, I would delve deeper into the modeling process. It would be particularly interesting to model the brands separately to enable direct comparisons, but I’ll save that exploration for another time, as this post is already quite extensive, but that was it!\nNote: As you may have noticed, exploratory data analysis has been a topic I’ve really enjoyed lately, and I plan to share more posts on this subject in the future.\n\n\n\n",
    "preview": "posts/2024-10-22-eda-clustering-data/eda-clustering-data_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-10-23T13:59:28-03:00",
    "input_file": "eda-clustering-data.knit.md"
  },
  {
    "path": "posts/2023-07-22-multiple-linear-regressions/",
    "title": "Multiple Linear Regressions",
    "description": "Quick analysis in pine trees",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2023-07-23",
    "categories": [],
    "contents": "\n \nMultiple LM by functional approach\n \nQuick analysis\n \n  Well, the idea here is to create multiple linear models in a straight forward analyzes to understand the relative changes between height in different ages and seeds of Pinus taeda, commonly known as loblolly pine, which is one of several pines native to the Southeastern United States. In order to do that I am going to apply a few statistical concepts to analyze the p-value and the estimates generated by the model, using the dataset called Loblolly, here it is the overview of it:\n\nTable 1: Data summary\nName\nLoblolly\nNumber of rows\n84\nNumber of columns\n3\n_______________________\n\nColumn type frequency:\n\nfactor\n1\nnumeric\n2\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\nSeed\n0\n1\nTRUE\n14\n329: 6, 327: 6, 325: 6, 307: 6\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nheight\n0\n1\n32.36\n20.67\n3.46\n10.47\n34.0\n51.36\n64.1\n▇▂▃▅▆\nage\n0\n1\n13.00\n7.90\n3.00\n5.00\n12.5\n20.00\n25.0\n▇▃▃▃▃\n\n\n\n\nObservation: I am aware that in a real world scenario we would need a bigger sample size in order to get reliable linear regression models, but here I am focused on studying the approach instead of checking all the assumptions required by the model.\n \nModels\n \n  I would like to go through somethings that I am considering here before jump to the modeling. So is important to mention that the p-value is a statistical measure that indicates the probability of obtaining the observed results, or even more extreme results, when the null hypothesis is true. In other words, it is a measure that helps us assess whether the available evidence suggests that an observed difference or effect is statistically significant or just a random result.\n  When we perform multiple statistical comparisons on the same dataset, we increase the probability of obtaining at least one significant result (i.e., a p-value smaller than the chosen significance level) by chance alone, even if there is no true effect present. This phenomenon is known as the “multiple testing problem” or “multiple comparisons problem”, which can be solved through p-value adjustment.\n  To illustrate the importance of p-value adjustment, consider an example where you conduct 20 independent hypothesis tests, each with a significance level of 0.05. The probability of obtaining at least one false positive (false significant result) is higher than 1 - (1 - 0.05)^20 ≈ 0.64, which is approximately 64%. This means that if you do not adjust the p-values, you run a relatively high risk of erroneously concluding that there are significant effects when they do not exist.\n  Therefore to address this problem, there are several adjustment techniques. On this analysis I am going to use the Bonferroni correction in which the p-values are multiplied by the number of comparisons. Controlling the overall significance level to reduce the risk of false positives and make multiple comparisons more reliable.\n  Besides that, I am not interested in the Intercepts generated, which represents the estimated age when the height is zero (makes no sense at this case). Only in the slopes, which represents the estimated change in height associated with a one-unit increase in age. These coefficients provide crucial information about the relationship between height and age across different Seeds. Thus, by analyzing the slopes, we can understand how the height of trees responds to changes in age.\nFinally in order to accomplish all that was mentioned this code was written:\n\n\ntidy_lm <- Loblolly %>%\n  nest( data = c( height, age ) ) %>%\n  mutate( model = map( data, ~ lm( height ~ age, data = .x ) ) )\n\nslopes <- tidy_lm %>% \n  mutate( coefs = map( model, tidy ) ) %>% \n  unnest( coefs ) %>% \n  filter( term == \"age\" ) %>% \n  mutate( p.value = p.adjust( p.value ) )\n\nslopes\n\n# A tibble: 14 × 8\n   Seed  data       model  term  estimate std.error statistic  p.value\n   <ord> <list>     <list> <chr>    <dbl>     <dbl>     <dbl>    <dbl>\n 1 301   <nfnGrpdD> <lm>   age       2.61     0.175      14.9 0.000691\n 2 303   <nfnGrpdD> <lm>   age       2.71     0.164      16.5 0.000691\n 3 305   <nfnGrpdD> <lm>   age       2.75     0.192      14.3 0.000691\n 4 307   <nfnGrpdD> <lm>   age       2.57     0.136      18.9 0.000560\n 5 309   <nfnGrpdD> <lm>   age       2.67     0.142      18.8 0.000560\n 6 311   <nfnGrpdD> <lm>   age       2.60     0.142      18.3 0.000560\n 7 315   <nfnGrpdD> <lm>   age       2.58     0.155      16.6 0.000691\n 8 319   <nfnGrpdD> <lm>   age       2.60     0.161      16.2 0.000691\n 9 321   <nfnGrpdD> <lm>   age       2.60     0.114      22.7 0.000310\n10 323   <nfnGrpdD> <lm>   age       2.65     0.182      14.5 0.000691\n11 325   <nfnGrpdD> <lm>   age       2.49     0.172      14.5 0.000691\n12 327   <nfnGrpdD> <lm>   age       2.42     0.146      16.6 0.000691\n13 329   <nfnGrpdD> <lm>   age       2.43     0.150      16.2 0.000691\n14 331   <nfnGrpdD> <lm>   age       2.57     0.134      19.2 0.000560\n\n \nResults\n\n\nslopes %>% \n  ggplot( aes( estimate, p.value, label = Seed ) ) +\n  geom_vline( xintercept = 0, lty = 2, linewidth = 0.9, alpha = 0.7, color = \"gray30\" ) +\n  geom_point( aes( color = Seed ), alpha = 0.8, size = 2.5, show.legend = FALSE ) +\n  facet_wrap( ~Seed ) +\n  labs( x = \"estimates\", title = \"Increase in height per age\" ) +\n  theme_bw( )  \n\n\n\nConclusion\n \n  The plot shows the relationship between the increase in height per age (slopes) for different levels of the Seed variable in the Loblolly dataset. Each facet represents a specific level of the Seed variable. The points on the plot represent the slopes for each seed. The dashed gray line at x = 0 represents the reference line where there is no increase or decrease in height with age.\nEverything is in the right side because as expected the height is increasing over the years. The interesting thing about this analysis is to understand that the further to the right it is, the larger the increase over this time period, enabling a quick overview between seeds. In summary we can highlight the 321 seed, due the low p-value and the high estimate.\nThe p-value measures the probability of obtaining results as extreme as those observed, assuming that the null hypothesis (no relationship or no change with time) is true. Lower p-values on the y-axis of the plot indicate stronger evidence against the null hypothesis, suggesting more confidence in the existence of real relationships between height and age which at this case is very obvious.\n  In conclusion is important to mention that this approach for using statistical models to estimate changes in many subgroups at once is very useful in different situations and can be applied to get reliable insights across the data.\n\n\n\n",
    "preview": "posts/2023-07-22-multiple-linear-regressions/multiple-linear-regressions_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-10-22T18:10:56-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-04-contributing-in-julia-language/",
    "title": "Contributing in Julia language",
    "description": "Solving a problem from Project Euler and contributing to a Julia package",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2023-03-04",
    "categories": [],
    "contents": "\n \n \nCoding challenge\n \nIntroduction\n \n  It’s been a while since my last post here, but I’ve really missed writing about my studies. So, I’ve decided to make a comeback! I’m excited to look back at my old posts and see how much progress I’ve made since I first started this blog.\n \n  At the time, I was just a student eager to learn new things in the R language. I’m proud to say that I landed my first job working solely on programming in R, where I spent an entire year honing my skills. However, since then, I’ve joined Uber as an Operations intern, which has given me the opportunity to learn outside the scope of programming. It’s been a fantastic experience, and I’m thoroughly enjoying the new challenges. And to top it off, 2023 marks my final year of graduation!\n \n  On this post I would like to discuss something I started a year ago and plan to continue doing, at least sporadically. Essentially I have been solving problems from Project Euler website, that is a platform for solving mathematical problems that typically require heavy procedures. The goal is to find a solution according to a “one-minute rule”, which means that although it may take several hours to design a successful algorithm with more difficult problems, an efficient implementation will allow a solution to be obtained on a modestly powered computer in less than one minute. I find it very intriguing and although I could not dedicate too much time on this, yet. I have been solving problems using mainly R language and sometimes Julia.\n \nProblem proposed\n \nNumber letter counts\n  If the numbers 1 to 5 are written out in words: one, two, three, four, five, then there are 3 + 3 + 5 + 4 + 4 = 19 letters used in total.\n  If all the numbers from 1 to 1000 (one thousand) inclusive were written out in words, how many letters would be used?\nNOTE: Do not count spaces or hyphens. For example, 342 (three hundred and forty-two) contains 23 letters and 115 (one hundred and fifteen) contains 20 letters. The use of “and” when writing out numbers is in compliance with British usage.\n \nSolving the Problem\n \n  I decided to solve this problem in Julia, so my line thinking was straightforward:\n“Transnumerate” and count their length for each iteration.\nOnce it is done, I just need to eliminate the white spaces and sum their length.\n  Therefore, in order to solve the first step I did a quick research on the clearnet where I could find a package called SpelledOut.jl from Jake. Which is a minimal package for a pure Julia implementation of converting numbers in their Arabic form to numbers in their Anglo-Saxan form. Exactly the first step required for the solution.\nThus, using this tool, I wrote the comprehension below to store each spelled out number on my vector v:\nv = [ SpelledOut.spelled_out( i, lang = :en_UK ) for i in 1:1000 ]\n  Now on the second step I need to find the length of each element in v, and then summing those lengths using the sum() function. Which is easily achieved using the following code:\nsum( length.( v ) )\n  Besides that, we need to find the amount of spaces and hyphens in the English word representations to subtract the previous result, which is determined by using the count() function with \" \" (space) and \"-\" (hyphen) as the patterns to count:\nsum( count.( \" \", v ) ) + sum( count.( \"-\", v ) )\n  Finally in the last step I could gather the whole process in a function:\nfunction total_letters( n::Int64 )\n  v = [ SpelledOut.spelled_out( i, lang = :en_UK ) for i in 1:n ]\n  sum( length.( v ) ) - ( sum( count.( \" \", v ) ) + sum( count.( \"-\", v ) ) )\nend\n\ntotal_letters( 1000 )\n  Getting the answer: 21124 🚀\n \nContributing\n \n  That was nice but quite easy with the help provided by this package. Therefore, I decided to also contribute with the package implementing a Portuguese version of it, including :pt_BR (Brazil) and :pt (Portugal). Which you can download and use from SpelledOut.jl repository.\n \nConclusion\n \n  I find the process of solving problems to be just as enjoyable as developing new features. Engaging in problem-solving exercises not only helps us to expand our knowledge and skill set but also trains our brain to think more critically and creatively. It keeps us motivated and focused on pursuing new challenges, which is essential for personal and professional growth. Whether it’s a coding problem or a mathematical puzzle, each challenge offers a unique opportunity to learn and improve. I truly believe that problem-solving is a fundamental skill that can benefit us in all aspects of life. Even my grandmother thinks the same thing when she solves her word search puzzles.\n\n\n\n",
    "preview": "https://projecteuler.net/images/clipart/euler_portrait.png",
    "last_modified": "2024-10-22T18:10:56-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-24-gas-emission-dashboard/",
    "title": "BR - Gas Emission Dashboard",
    "description": "My first dashboard on Power BI! \nUsing a query of gas emission dataset from (https://basedosdados.org/) to visualize the situation of several states for 2018-2019",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-07-25",
    "categories": [],
    "contents": "\n \n \n\nQuerying and visualizing data\n\n \n \nIntroduction\n \n  Currently I am on vacation from university so I’ve started my studies on SQL through SQL Server desktop. After the beginning learning process, when I learned the structure to create databases and became myself able to make some queries, I decided to make a project with real data instead just working with simple databases created by my own. Besides that, I tried to gather both beginner knowledges of SQL and Power BI to use the data to make a dashboard. Although I am new using these tools, this is not my first time doing a dashboard, I’ve made it through R language and you can check an example of it through this repository which also includes a github page as I am used to do on my github.\n \n  After this period learning the basics (which was quite fast because the “SEQUEL” is very similar with English and the package dplyr from R, which probably is based on SQL) I’ve started to practice it using the Google BigQuery which is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. And I would like to comment that It was very impressive for me how it works so fast using an amount of data that my computer would never support…\n \n  Anyways I have been using it a lot in order to practice SQL and generate quickly insights about different interesting subjects through Data Studio (which I am studying as well) within the same platform. The basedosdados initiative to gather the public information from Brazil in a well organized way is very useful to combine several datasets through BigQuery and produce valuable and palpable perspectives about the content produced around us. That’s why I chose a dataset available there, to get my first dashboard on Power BI, possibly divulging this organization for mates that are reading this.\n \n  I have written this following query to get my data which is totally reproducible thanks to basedosdados:\n-- gas emission of CO2e (t) GWP-AR5 \n-- table joined with population data from IBGE by state for 2018 and 2019\n-- considering 4 different sectors.\n\nWITH \nt1 AS(\nSELECT \n  ano,\n  nivel_1 AS setor,\n  sigla_uf AS estado,\n  gas,\n  SUM(emissao) AS emissao\nFROM \n  `basedosdados.br_seeg_emissoes.uf` \nWHERE \n  ano = 2018 OR ano = 2019\n  AND gas = 'CO2e (t) GWP-AR5'\nGROUP BY\n  setor, estado, ano, gas\n),\nt2 AS(\nSELECT \n  ano,\n  nivel_1 AS setor,\n  sigla_uf AS estado\nFROM\n  `basedosdados.br_seeg_emissoes.uf`\nWHERE\n  ano = 2018 OR ano = 2019\n  AND gas = 'CO2e (t) GWP-AR5'\nGROUP BY\n  ano, setor, estado\n),\nt3 AS(\nSELECT \n  ano, \n  sigla_uf, \n  populacao\nFROM\n  `basedosdados.br_ibge_populacao.uf`\nWHERE\n  ano = 2018 OR ano = 2019\nGROUP BY \n  sigla_uf, ano, populacao\n),\nt4 AS(\nSELECT \n  t2.ano,\n  t2.setor,\n  t2.estado,\n  t3.populacao\nFROM \n  t2\nJOIN\n  t3 \n  ON t2.ano = t3.ano\n  AND t2.estado = t3.sigla_uf \n)\nSELECT\n  t4.ano,\n  t4.setor,\n  t4.estado,\n  t4.populacao,\n  t1.gas,\n  t1.emissao\nFROM \n  t4\nJOIN\n  t1\n  ON t4.ano = t1.ano\n  AND t4.setor = t1.setor\n  AND t4.estado = t1.estado\nWHERE \n  gas = 'CO2e (t) GWP-AR5'\n \n \nQueried Data\n   \n  YEARS: the years covered on the query were 2018 and 2019.  \n  POPULATION: population of 2018-2019.  \n  STATES: all the states from Brazil.  \n  SECTORS: the data has 5 sectors and each one cover a determined area as specified below  \nagriculture and livestock\nrice cultivation\nenteric fermentation\nanimal waste management\nburning agricultural waste\nmanaged soils\nenergy\nfugitive emissions\nemissions from fuel combustion\nwaste\nliquid effluents\nsolid waste\nindustrial processes\nHFCs emissions\nchemical industry\nminearl products\nmetal production\nnon-energy use of fuels and use of solvents\nuse of SF6\nland and forest use change\nland use changes\nforest residues\n    EMISSION: the gas that I’ve selected to compose this field is the equivalent carbon (t) GWP-AR5. Is important to say that there are two main approaches to determining the equivalent carbon: GWP (Global Warming Potential) and GTP (Global Temperature Change Potential). The first one considers the influence of the gases in the alteration of the energy balance of the Earth and, second, the influence in the increase of temperature. Both are measured for a term of 100 years, with GWP being most commonly used and the ‘AR5’ refers to the total emissions of greenhouse gases which were computed in the fith inventory defined by the Intergovernmental Panel on Climate Change (IPCC) guidelines for national inventories, that’s why I chose this one. You can check more information about the data at this link.\n     \nDashboard\n   \n\n\nConclusion\n   \nI really enjoy the process to make this project learning SQL and Power Bi together which definitely allows me to spread my analyses combining several tools and keep learning it at the same time.\nTo finish, I would like to share the DAX formula that I’ve used to get the population by year and state on my dashboard:\n   \npop 2019 = \n\nCALCULATE ( \n  \n  SUMX ( \n  \n    VALUES ( 'sql_gas_emission_2 (2)'[estado] ),\n    \n    CALCULATE( \n      MAX ( 'sql_gas_emission_2 (2)'[populacao] ) \n        ) \n  \n  ), \n  \n'sql_gas_emission_2 (2)'[ano] = 2019\n) \n\n\n\n",
    "preview": "https://github.com/Gabrielforest/portfolio/blob/main/dash_img.PNG?raw=true",
    "last_modified": "2024-10-22T18:10:56-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-18-predicting-na-values/",
    "title": "Predicting Missing Values",
    "description": "Analising forest inventory data and predicting NAs through least squares regression model",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-04-18",
    "categories": [],
    "contents": "\nIntroduction\n   \n  This data was provided by my dendrometry teacher and it is the result of a sistematic collection of data that was carried out between 2017 and 2018 at Três Lagoas, which is a city from Mato Grosso do Sul state. The area has an extent of 1226 acres of Eucalyptus grandis W. Hill ex Maiden x Eucalyptus urophylla S. T. Blake clonal culture with mean age of 2.88 years.\n\n\n\nExploring the data\n   \n  Here we can take a look on the first 6 rows from the data frame which has 3313 obs. of 6 variables:\n\n\narea(ha)\n\n\nspacing\n\n\ngenetic material\n\n\nage\n\n\nDBH\n\n\nheigth\n\n\n42.43\n\n\n3,45X2,40\n\n\n35\n\n\n2.839151\n\n\n12.16\n\n\n16.98621\n\n\n42.43\n\n\n3,45X2,40\n\n\n35\n\n\n2.839151\n\n\n11.52\n\n\n16.61542\n\n\n42.43\n\n\n3,45X2,40\n\n\n35\n\n\n2.839151\n\n\n12.00\n\n\n16.89602\n\n\n42.43\n\n\n3,45X2,40\n\n\n35\n\n\n2.839151\n\n\n11.87\n\n\n16.82587\n\n\n42.43\n\n\n3,45X2,40\n\n\n35\n\n\n2.839151\n\n\n11.33\n\n\n16.49516\n\n\n42.43\n\n\n3,45X2,40\n\n\n35\n\n\n2.839151\n\n\n12.45\n\n\n17.13653\n\n\n   \nChecking whether we have got NAs or not:\n\n\nsum(is.na(fustes_6$DBH))\n\n\n[1] 134\n\n   \n  As we can see there are 134 NAs from DBH values that probably represent dead trees, that can be replanted in the future for next rotation. But just to be sure about it, let’s take a look on the age! Because those NAs values also could be trees too much young for DBH.\n\n\nmin(fustes_6$age)\n\n\n[1] 2.811773\n\n   \n  The minimum age turns out the idea of NAs are representing dead trees. Therefore if we would like to get a second rotation forestry would be good to understand the behavior of trees under similar conditions to do future economic analysis. So what would we expect from these trees if they were alive?  \n  We can make an approximation through least squares regression model based on our data and variables to find out their diameter at breast height! And for that we need to pre-process the data…    \nData pre-processing\n   \nChecking whether we have got outliers on DBH or not:\n\n\n\n  As we can see, there are outliers on the data that we want to remove because it’s not good for least squares regression model. The result bellow show us the lower extreme, first quartile, median, third quartile and the upper extreme respectively.\n\n\nboxplot.stats(fustes_6$DBH)$stats\n\n\n[1]  8.820 11.585 12.510 13.430 16.180\n\n   \nAutomating the process to get the extreme values:\n\n\nupper_whisker <- boxplot.stats(fustes_6$DBH)$stats[5]\nlower_whisker <- boxplot.stats(fustes_6$DBH)$stats[1]\n\n\n\n   \nOf course after the upper extreme we’ve got the outliers and before the first whisker as well. And both we are going to remove this way:\n\n\noutlier_filter_upper <- fustes_6$DBH < upper_whisker \noutlier_filter_lower <- fustes_6$DBH > lower_whisker \n\n#filtering outliers: \noutlier_filter <- fustes_6[outlier_filter_lower & outlier_filter_upper,]\n\n\n\n   \nNow we need to check the class from the variables and if necessary make some changes…\n\n\nstr(fustes_6)\n\n\n'data.frame':   3313 obs. of  6 variables:\n $ area(ha)        : num  42.4 42.4 42.4 42.4 42.4 ...\n $ spacing         : chr  \"3,45X2,40\" \"3,45X2,40\" \"3,45X2,40\" \"3,45X2,40\" ...\n $ genetic material: int  35 35 35 35 35 35 35 35 35 35 ...\n $ age             : num  2.84 2.84 2.84 2.84 2.84 ...\n $ DBH             : num  12.2 11.5 12 11.9 11.3 ...\n $ heigth          : num  17 16.6 16.9 16.8 16.5 ...\n\n   \nCategorical casting variables that we will use to predict:\n\n\nfustes_6$`genetic material` <- as.factor(fustes_6$`genetic material`)\n\n\n\n   \nBuilding the model\n   \nOur formula has the following format: \\(\\hat{y}=\\hat{\\beta_0}+\\hat{\\beta_1}X_1+\\hat{\\beta_2}X_2\\)\n\n\ndbh_equation <- \"DBH ~ `genetic material` + age\"\n\n\n\n   \nPlotting these variables:\n\n\n\n   \nRegression model:\n\n\n#removing NAs to make the model:\nfustes_model <- na.omit(fustes_6)\n\ndbh_model <- lm(\n  formula = dbh_equation, \n  data = fustes_model[outlier_filter_lower & outlier_filter_upper,]\n)\n\nsummary(dbh_model)\n\n\n\nCall:\nlm(formula = dbh_equation, data = fustes_model[outlier_filter_lower & \n    outlier_filter_upper, ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6626 -0.7964  0.1341  1.0160  6.8762 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           3.87366    2.40738   1.609 0.107708    \n`genetic material`12 -0.52911    0.11485  -4.607 4.26e-06 ***\n`genetic material`19  0.12014    0.17866   0.672 0.501359    \n`genetic material`35 -0.20092    0.08401  -2.392 0.016838 *  \n`genetic material`60 -0.26753    0.09896  -2.703 0.006905 ** \nage                   3.01675    0.82678   3.649 0.000268 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.619 on 2912 degrees of freedom\n  (264 observations deleted due to missingness)\nMultiple R-squared:  0.02809,   Adjusted R-squared:  0.02643 \nF-statistic: 16.84 on 5 and 2912 DF,  p-value: < 2.2e-16\n\n   \nTherefore, as we could see above the age and genetic material are going to be useful variables on the predictions of DBH through this model!\n   \nNow we are going to isolate the NA values (that we want to predict) and all the columns that we are using to get DBH values:\n\n\ndbh_rows <- fustes_6[is.na(fustes_6$DBH), c(\"genetic material\",\"age\")]\n\n\n\n   \nThe last step is to predict and replace those missing values:\n\n\ndbh_predictions <- predict(dbh_model, newdata = dbh_rows)\ndbh_predictions\n\n\n      22       80      122      163      175      181      187 \n12.23775 12.17994 11.93434 11.93434 11.93434 12.72776 12.72776 \n     209      219      324      328      357      378      421 \n12.72776 12.72776 11.87653 11.87653 12.08301 12.08301 12.78557 \n     448      455      564      576      588      601      602 \n12.78557 12.78557 12.60387 12.55431 12.55431 12.55431 12.55431 \n     608      627      628      686      694      695      718 \n12.55431 12.44424 12.44424 12.62038 12.62038 12.62038 12.62038 \n     776      793      831      859      866      869      879 \n12.19592 12.16342 12.16342 12.33633 12.33633 12.33633 12.33633 \n     886      887      889      891      905      962      990 \n12.33633 12.33633 12.33633 12.33633 12.39415 12.61663 12.61663 \n    1009     1027     1036     1045     1057     1060     1124 \n12.54605 12.54605 12.54605 12.54605 12.54605 12.54605 12.19646 \n    1211     1212     1281     1291     1292     1293     1352 \n12.56257 12.56257 12.32861 12.79383 12.79383 12.79383 12.42772 \n    1427     1471     1475     1479     1480     1481     1490 \n12.76079 12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 \n    1491     1496     1498     1501     1503     1504     1509 \n12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 \n    1563     1624     1626     1627     1628     1629     1630 \n12.61769 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \n    1631     1636     1642     1643     1646     1647     1648 \n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \n    1649     1650     1651     1660     1662     1663     1664 \n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \n    1665     1668     1669     1670     1671     1676     1677 \n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \n    1678     1679     1711     1748     1777     1781     1782 \n12.79383 12.79383 11.84349 11.85175 11.85175 11.85175 11.85175 \n    1788     1791     1794     1873     1886     1923     1924 \n11.85175 11.85175 11.85175 12.21297 12.21297 12.22123 12.22123 \n    1942     1943     1967     1973     1982     2003     2006 \n12.22123 12.22123 12.22123 12.73602 12.73602 12.73602 12.73602 \n    2007     2042     2043     2090     2095     2139     2145 \n12.73602 12.60387 12.60387 12.22070 12.22070 12.71950 12.71950 \n    2200     2201     2208     2226     2400     2626     2799 \n12.18820 12.18820 12.18820 12.18820 12.55431 12.25427 12.71124 \n    2826 \n12.71124 \n\nFilling the data with predicted values:\n\n\nfustes_6[is.na(fustes_6$DBH), \"DBH\"] <- dbh_predictions\n\n\n\n\n\n\n",
    "preview": "posts/2021-04-18-predicting-na-values/predicting-na-values_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2024-10-22T18:10:56-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-18-visualizing-maps-interactively/",
    "title": "Visualizing Maps Interactively",
    "description": "My first shiny app! \nUsing the results from ensemble models of my scientific initiation\nto define the priority areas for conservation of Brazil nut",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-02-18",
    "categories": [],
    "contents": "\n \n \n\nVisualizing maps in a simple way\n\n \n \nIntroduction\n   \n  First of all, what made me get started and learn shiny was the facility provided by the app for visualization. This is quite useful when you’ve got an amount of data to visualize and compare. Of course any GIS software is enough to do this job! But it’s not easy to handle and share a set of maps keeping the interactivity from them so that’s why I was searching how to make it properly. And everyone that I know would rather enjoy beautiful interactive graphics or maps to analyse the data. I think this is the best way to deal with the results, it is always good to simplify the visualization before studying it further theoretically.  \n  Well, the maps analysed were results from a prediction made by ensemble models to find out the priority areas for conservation of Bertholletia excelsa using different filters (environmental and geographical) based on 15 variables defined as the best in predicting the suitability of Brazil nut by Daiana tourne et. al 2019 on her doctorate degree using principal component analyses (PCA) combined with expert analyses.  \n  Therefore the main objective is directly correlated with the climatic analyses and soon we will combine with genetic analyses. These results presented here are considering the actual conditions using the variables from different databases as Worldclim, USGS, etc. More details about the research will be posted in a formal article in the future when it gets ready. Is important to say that in this research I have got 3 co-authors from Bioversity International which are: Evert Thomas, Tobias Fremout and Viviana Ceccarelli. Besides them Daiana also is helping in this project as well as my advisor Karina Martins.  \n  The shiny app has a good and organized structure quite easy to understand which helped me a lot to generate the app. However I didn’t use packages as Golem to provide a code cleaner so it was a robust script which I have written without using modules, because although the app has a clear structure and easy to apply modules I wasn’t familiar with it.\n \n \nExploring the maps\n   \n  The app has 6 tabs with different filters and content as you can see below:\nSuitability - Environmental filtering\nCount - Environmental filtering\nBinary - Environmental filtering\nSuitability - Geographical filtering\nCount - Geographical filtering\nBinary - Geographical filtering\n\n\n  So as I said before the model is an ensemble of different algorithms (MAXENT, RF, GLMNET, etc). Each of these algorithms produces a different presence map. The suitability map shows us the probability (0-1000) to get the presence of the species occurring at the site assuming a specific threshold based on the model filter which could be Environmental or Geographical. The count map indicates how many algorithms predict suitable a point, from 0 algorithms to 8 algorithms which is the maximum number of algorithms which predict suitable the same grid from 10 algorithms used in the ensemble initially. And last but not least the binary map shows us absence and presence (0-1) also considering the same threshold for suitability map.    \nI will leave here the Shiny app if you would like to take a look further.\n\n\n\n",
    "preview": "https://github.com/Gabrielforest/portfolio/blob/main/previewblog.PNG?raw=true",
    "last_modified": "2024-10-22T18:10:56-03:00",
    "input_file": {}
  }
]
