[
  {
    "path": "posts/2023-07-22-multiple-linear-regressions/",
    "title": "Multiple Linear Regressions",
    "description": "Quick analysis in pine trees",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2023-07-23",
    "categories": [],
    "contents": "\r\n \r\nMultiple LM by functional approach\r\n \r\nQuick analysis\r\n \r\n  Well, the idea here is to create multiple linear models in a straight forward analyzes to understand the relative changes between height in different ages and seeds of Pinus taeda, commonly known as loblolly pine, which is one of several pines native to the Southeastern United States. In order to do that I am going to apply a few statistical concepts to analyze the p-value and the estimates generated by the model, using the dataset called Loblolly, here it is the overview of it:\r\n\r\nTable 1: Data summary\r\nName\r\nLoblolly\r\nNumber of rows\r\n84\r\nNumber of columns\r\n3\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n1\r\nnumeric\r\n2\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nSeed\r\n0\r\n1\r\nTRUE\r\n14\r\n329: 6, 327: 6, 325: 6, 307: 6\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nheight\r\n0\r\n1\r\n32.36\r\n20.67\r\n3.46\r\n10.47\r\n34.0\r\n51.36\r\n64.1\r\n▇▂▃▅▆\r\nage\r\n0\r\n1\r\n13.00\r\n7.90\r\n3.00\r\n5.00\r\n12.5\r\n20.00\r\n25.0\r\n▇▃▃▃▃\r\n\r\n\r\n\r\n\r\nObservation: I am aware that in a real world scenario we would need a bigger sample size in order to get reliable linear regression models, but here I am focused on studying the approach instead of checking all the assumptions required by the model.\r\n \r\nModels\r\n \r\n  I would like to go through somethings that I am considering here before jump to the modeling. So is important to mention that the p-value is a statistical measure that indicates the probability of obtaining the observed results, or even more extreme results, when the null hypothesis is true. In other words, it is a measure that helps us assess whether the available evidence suggests that an observed difference or effect is statistically significant or just a random result.\r\n  When we perform multiple statistical comparisons on the same dataset, we increase the probability of obtaining at least one significant result (i.e., a p-value smaller than the chosen significance level) by chance alone, even if there is no true effect present. This phenomenon is known as the “multiple testing problem” or “multiple comparisons problem”, which can be solved through p-value adjustment.\r\n  To illustrate the importance of p-value adjustment, consider an example where you conduct 20 independent hypothesis tests, each with a significance level of 0.05. The probability of obtaining at least one false positive (false significant result) is higher than 1 - (1 - 0.05)^20 ≈ 0.64, which is approximately 64%. This means that if you do not adjust the p-values, you run a relatively high risk of erroneously concluding that there are significant effects when they do not exist.\r\n  Therefore to address this problem, there are several adjustment techniques. On this analysis I am going to use the Bonferroni correction in which the p-values are multiplied by the number of comparisons. Controlling the overall significance level to reduce the risk of false positives and make multiple comparisons more reliable.\r\n  Besides that, I am not interested in the Intercepts generated, which represents the estimated age when the height is zero (makes no sense at this case). Only in the slopes, which represents the estimated change in height associated with a one-unit increase in age. These coefficients provide crucial information about the relationship between height and age across different Seeds. Thus, by analyzing the slopes, we can understand how the height of trees responds to changes in age.\r\nFinally in order to accomplish all that was mentioned this code was written:\r\n\r\n\r\ntidy_lm <- Loblolly %>%\r\n  nest( data = c( height, age ) ) %>%\r\n  mutate( model = map( data, ~ lm( height ~ age, data = .x ) ) )\r\n\r\nslopes <- tidy_lm %>% \r\n  mutate( coefs = map( model, tidy ) ) %>% \r\n  unnest( coefs ) %>% \r\n  filter( term == \"age\" ) %>% \r\n  mutate( p.value = p.adjust( p.value ) )\r\n\r\nslopes\r\n\r\n# A tibble: 14 × 8\r\n   Seed  data       model  term  estimate std.error statistic  p.value\r\n   <ord> <list>     <list> <chr>    <dbl>     <dbl>     <dbl>    <dbl>\r\n 1 301   <nfnGrpdD> <lm>   age       2.61     0.175      14.9 0.000691\r\n 2 303   <nfnGrpdD> <lm>   age       2.71     0.164      16.5 0.000691\r\n 3 305   <nfnGrpdD> <lm>   age       2.75     0.192      14.3 0.000691\r\n 4 307   <nfnGrpdD> <lm>   age       2.57     0.136      18.9 0.000560\r\n 5 309   <nfnGrpdD> <lm>   age       2.67     0.142      18.8 0.000560\r\n 6 311   <nfnGrpdD> <lm>   age       2.60     0.142      18.3 0.000560\r\n 7 315   <nfnGrpdD> <lm>   age       2.58     0.155      16.6 0.000691\r\n 8 319   <nfnGrpdD> <lm>   age       2.60     0.161      16.2 0.000691\r\n 9 321   <nfnGrpdD> <lm>   age       2.60     0.114      22.7 0.000310\r\n10 323   <nfnGrpdD> <lm>   age       2.65     0.182      14.5 0.000691\r\n11 325   <nfnGrpdD> <lm>   age       2.49     0.172      14.5 0.000691\r\n12 327   <nfnGrpdD> <lm>   age       2.42     0.146      16.6 0.000691\r\n13 329   <nfnGrpdD> <lm>   age       2.43     0.150      16.2 0.000691\r\n14 331   <nfnGrpdD> <lm>   age       2.57     0.134      19.2 0.000560\r\n\r\n \r\nResults\r\n\r\n\r\nslopes %>% \r\n  ggplot( aes( estimate, p.value, label = Seed ) ) +\r\n  geom_vline( xintercept = 0, lty = 2, linewidth = 0.9, alpha = 0.7, color = \"gray30\" ) +\r\n  geom_point( aes( color = Seed ), alpha = 0.8, size = 2.5, show.legend = FALSE ) +\r\n  facet_wrap( ~Seed ) +\r\n  labs( x = \"estimates\", title = \"Increase in height per age\" ) +\r\n  theme_bw( )  \r\n\r\n\r\n\r\nConclusion\r\n \r\n  The plot shows the relationship between the increase in height per age (slopes) for different levels of the Seed variable in the Loblolly dataset. Each facet represents a specific level of the Seed variable. The points on the plot represent the slopes for each seed. The dashed gray line at x = 0 represents the reference line where there is no increase or decrease in height with age.\r\nEverything is in the right side because as expected the height is increasing over the years. The interesting thing about this analysis is to understand that the further to the right it is, the larger the increase over this time period, enabling a quick overview between seeds. In summary we can highlight the 321 seed, due the low p-value and the high estimate.\r\nThe p-value measures the probability of obtaining results as extreme as those observed, assuming that the null hypothesis (no relationship or no change with time) is true. Lower p-values on the y-axis of the plot indicate stronger evidence against the null hypothesis, suggesting more confidence in the existence of real relationships between height and age which at this case is very obvious.\r\n  In conclusion is important to mention that this approach for using statistical models to estimate changes in many subgroups at once is very useful in different situations and can be applied to get reliable insights across the data.\r\n\r\n\r\n\r\n",
    "preview": "posts/2023-07-22-multiple-linear-regressions/multiple-linear-regressions_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-08-12T13:30:35-03:00",
    "input_file": "multiple-linear-regressions.knit.md"
  },
  {
    "path": "posts/2023-03-04-contributing-in-julia-language/",
    "title": "Contributing in Julia language",
    "description": "Solving a problem from Project Euler and contributing to a Julia package",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2023-03-04",
    "categories": [],
    "contents": "\r\n \r\n \r\nCoding challenge\r\n \r\nIntroduction\r\n \r\n  It’s been a while since my last post here, but I’ve really missed writing about my studies. So, I’ve decided to make a comeback! I’m excited to look back at my old posts and see how much progress I’ve made since I first started this blog.\r\n \r\n  At the time, I was just a student eager to learn new things in the R language. I’m proud to say that I landed my first job working solely on programming in R, where I spent an entire year honing my skills. However, since then, I’ve joined Uber as an Operations intern, which has given me the opportunity to learn outside the scope of programming. It’s been a fantastic experience, and I’m thoroughly enjoying the new challenges. And to top it off, 2023 marks my final year of graduation!\r\n \r\n  On this post I would like to discuss something I started a year ago and plan to continue doing, at least sporadically. Essentially I have been solving problems from Project Euler website, that is a platform for solving mathematical problems that typically require heavy procedures. The goal is to find a solution according to a “one-minute rule”, which means that although it may take several hours to design a successful algorithm with more difficult problems, an efficient implementation will allow a solution to be obtained on a modestly powered computer in less than one minute. I find it very intriguing and although I could not dedicate too much time on this, yet. I have been solving problems using mainly R language and sometimes Julia.\r\n \r\nProblem proposed\r\n \r\nNumber letter counts\r\n  If the numbers 1 to 5 are written out in words: one, two, three, four, five, then there are 3 + 3 + 5 + 4 + 4 = 19 letters used in total.\r\n  If all the numbers from 1 to 1000 (one thousand) inclusive were written out in words, how many letters would be used?\r\nNOTE: Do not count spaces or hyphens. For example, 342 (three hundred and forty-two) contains 23 letters and 115 (one hundred and fifteen) contains 20 letters. The use of “and” when writing out numbers is in compliance with British usage.\r\n \r\nSolving the Problem\r\n \r\n  I decided to solve this problem in Julia, so my line thinking was straightforward:\r\n“Transnumerate” and count their length for each iteration.\r\nOnce it is done, I just need to eliminate the white spaces and sum their length.\r\n  Therefore, in order to solve the first step I did a quick research on the clearnet where I could find a package called SpelledOut.jl from Jake. Which is a minimal package for a pure Julia implementation of converting numbers in their Arabic form to numbers in their Anglo-Saxan form. Exactly the first step required for the solution.\r\nThus, using this tool, I wrote the comprehension below to store each spelled out number on my vector v:\r\nv = [ SpelledOut.spelled_out( i, lang = :en_UK ) for i in 1:1000 ]\r\n  Now on the second step I need to find the length of each element in v, and then summing those lengths using the sum() function. Which is easily achieved using the following code:\r\nsum( length.( v ) )\r\n  Besides that, we need to find the amount of spaces and hyphens in the English word representations to subtract the previous result, which is determined by using the count() function with \" \" (space) and \"-\" (hyphen) as the patterns to count:\r\nsum( count.( \" \", v ) ) + sum( count.( \"-\", v ) )\r\n  Finally in the last step I could gather the whole process in a function:\r\nfunction total_letters( n::Int64 )\r\n  v = [ SpelledOut.spelled_out( i, lang = :en_UK ) for i in 1:n ]\r\n  sum( length.( v ) ) - ( sum( count.( \" \", v ) ) + sum( count.( \"-\", v ) ) )\r\nend\r\n\r\ntotal_letters( 1000 )\r\n  Getting the answer: 21124 🚀\r\n \r\nContributing\r\n \r\n  That was nice but quite easy with the help provided by this package. Therefore, I decided to also contribute with the package implementing a Portuguese version of it, including :pt_BR (Brazil) and :pt (Portugal). Which you can download and use from SpelledOut.jl repository.\r\n \r\nConclusion\r\n \r\n  I find the process of solving problems to be just as enjoyable as developing new features. Engaging in problem-solving exercises not only helps us to expand our knowledge and skill set but also trains our brain to think more critically and creatively. It keeps us motivated and focused on pursuing new challenges, which is essential for personal and professional growth. Whether it’s a coding problem or a mathematical puzzle, each challenge offers a unique opportunity to learn and improve. I truly believe that problem-solving is a fundamental skill that can benefit us in all aspects of life. Even my grandmother thinks the same thing when she solves her word search puzzles.\r\n\r\n\r\n\r\n",
    "preview": "https://projecteuler.net/images/clipart/euler_portrait.png",
    "last_modified": "2023-03-04T19:17:23-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-24-gas-emission-dashboard/",
    "title": "BR - Gas Emission Dashboard",
    "description": "My first dashboard on Power BI! \nUsing a query of gas emission dataset from (https://basedosdados.org/) to visualize the situation of several states for 2018-2019",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-07-25",
    "categories": [],
    "contents": "\r\n \r\n \r\n\r\nQuerying and visualizing data\r\n\r\n \r\n \r\nIntroduction\r\n \r\n  Currently I am on vacation from university so I’ve started my studies on SQL through SQL Server desktop. After the beginning learning process, when I learned the structure to create databases and became myself able to make some queries, I decided to make a project with real data instead just working with simple databases created by my own. Besides that, I tried to gather both beginner knowledges of SQL and Power BI to use the data to make a dashboard. Although I am new using these tools, this is not my first time doing a dashboard, I’ve made it through R language and you can check an example of it through this repository which also includes a github page as I am used to do on my github.\r\n \r\n  After this period learning the basics (which was quite fast because the “SEQUEL” is very similar with English and the package dplyr from R, which probably is based on SQL) I’ve started to practice it using the Google BigQuery which is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. And I would like to comment that It was very impressive for me how it works so fast using an amount of data that my computer would never support…\r\n \r\n  Anyways I have been using it a lot in order to practice SQL and generate quickly insights about different interesting subjects through Data Studio (which I am studying as well) within the same platform. The basedosdados initiative to gather the public information from Brazil in a well organized way is very useful to combine several datasets through BigQuery and produce valuable and palpable perspectives about the content produced around us. That’s why I chose a dataset available there, to get my first dashboard on Power BI, possibly divulging this organization for mates that are reading this.\r\n \r\n  I have written this following query to get my data which is totally reproducible thanks to basedosdados:\r\n-- gas emission of CO2e (t) GWP-AR5 \r\n-- table joined with population data from IBGE by state for 2018 and 2019\r\n-- considering 4 different sectors.\r\n\r\nWITH \r\nt1 AS(\r\nSELECT \r\n  ano,\r\n  nivel_1 AS setor,\r\n  sigla_uf AS estado,\r\n  gas,\r\n  SUM(emissao) AS emissao\r\nFROM \r\n  `basedosdados.br_seeg_emissoes.uf` \r\nWHERE \r\n  ano = 2018 OR ano = 2019\r\n  AND gas = 'CO2e (t) GWP-AR5'\r\nGROUP BY\r\n  setor, estado, ano, gas\r\n),\r\nt2 AS(\r\nSELECT \r\n  ano,\r\n  nivel_1 AS setor,\r\n  sigla_uf AS estado\r\nFROM\r\n  `basedosdados.br_seeg_emissoes.uf`\r\nWHERE\r\n  ano = 2018 OR ano = 2019\r\n  AND gas = 'CO2e (t) GWP-AR5'\r\nGROUP BY\r\n  ano, setor, estado\r\n),\r\nt3 AS(\r\nSELECT \r\n  ano, \r\n  sigla_uf, \r\n  populacao\r\nFROM\r\n  `basedosdados.br_ibge_populacao.uf`\r\nWHERE\r\n  ano = 2018 OR ano = 2019\r\nGROUP BY \r\n  sigla_uf, ano, populacao\r\n),\r\nt4 AS(\r\nSELECT \r\n  t2.ano,\r\n  t2.setor,\r\n  t2.estado,\r\n  t3.populacao\r\nFROM \r\n  t2\r\nJOIN\r\n  t3 \r\n  ON t2.ano = t3.ano\r\n  AND t2.estado = t3.sigla_uf \r\n)\r\nSELECT\r\n  t4.ano,\r\n  t4.setor,\r\n  t4.estado,\r\n  t4.populacao,\r\n  t1.gas,\r\n  t1.emissao\r\nFROM \r\n  t4\r\nJOIN\r\n  t1\r\n  ON t4.ano = t1.ano\r\n  AND t4.setor = t1.setor\r\n  AND t4.estado = t1.estado\r\nWHERE \r\n  gas = 'CO2e (t) GWP-AR5'\r\n \r\n \r\nQueried Data\r\n   \r\n  YEARS: the years covered on the query were 2018 and 2019.  \r\n  POPULATION: population of 2018-2019.  \r\n  STATES: all the states from Brazil.  \r\n  SECTORS: the data has 5 sectors and each one cover a determined area as specified below  \r\nagriculture and livestock\r\nrice cultivation\r\nenteric fermentation\r\nanimal waste management\r\nburning agricultural waste\r\nmanaged soils\r\nenergy\r\nfugitive emissions\r\nemissions from fuel combustion\r\nwaste\r\nliquid effluents\r\nsolid waste\r\nindustrial processes\r\nHFCs emissions\r\nchemical industry\r\nminearl products\r\nmetal production\r\nnon-energy use of fuels and use of solvents\r\nuse of SF6\r\nland and forest use change\r\nland use changes\r\nforest residues\r\n    EMISSION: the gas that I’ve selected to compose this field is the equivalent carbon (t) GWP-AR5. Is important to say that there are two main approaches to determining the equivalent carbon: GWP (Global Warming Potential) and GTP (Global Temperature Change Potential). The first one considers the influence of the gases in the alteration of the energy balance of the Earth and, second, the influence in the increase of temperature. Both are measured for a term of 100 years, with GWP being most commonly used and the ‘AR5’ refers to the total emissions of greenhouse gases which were computed in the fith inventory defined by the Intergovernmental Panel on Climate Change (IPCC) guidelines for national inventories, that’s why I chose this one. You can check more information about the data at this link.\r\n     \r\nDashboard\r\n   \r\n\r\n\r\nConclusion\r\n   \r\nI really enjoy the process to make this project learning SQL and Power Bi together which definitely allows me to spread my analyses combining several tools and keep learning it at the same time.\r\nTo finish, I would like to share the DAX formula that I’ve used to get the population by year and state on my dashboard:\r\n   \r\npop 2019 = \r\n\r\nCALCULATE ( \r\n  \r\n  SUMX ( \r\n  \r\n    VALUES ( 'sql_gas_emission_2 (2)'[estado] ),\r\n    \r\n    CALCULATE( \r\n      MAX ( 'sql_gas_emission_2 (2)'[populacao] ) \r\n        ) \r\n  \r\n  ), \r\n  \r\n'sql_gas_emission_2 (2)'[ano] = 2019\r\n) \r\n\r\n\r\n\r\n",
    "preview": "https://github.com/Gabrielforest/portfolio/blob/main/dash_img.PNG?raw=true",
    "last_modified": "2023-03-04T14:59:42-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-18-predicting-na-values/",
    "title": "Predicting Missing Values",
    "description": "Analising forest inventory data and predicting NAs through least squares regression model",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-04-18",
    "categories": [],
    "contents": "\r\nIntroduction\r\n   \r\n  This data was provided by my dendrometry teacher and it is the result of a sistematic collection of data that was carried out between 2017 and 2018 at Três Lagoas, which is a city from Mato Grosso do Sul state. The area has an extent of 1226 acres of Eucalyptus grandis W. Hill ex Maiden x Eucalyptus urophylla S. T. Blake clonal culture with mean age of 2.88 years.\r\n\r\n\r\n\r\nExploring the data\r\n   \r\n  Here we can take a look on the first 6 rows from the data frame which has 3313 obs. of 6 variables:\r\n\r\n\r\narea(ha)\r\n\r\n\r\nspacing\r\n\r\n\r\ngenetic material\r\n\r\n\r\nage\r\n\r\n\r\nDBH\r\n\r\n\r\nheigth\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n12.16\r\n\r\n\r\n16.98621\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n11.52\r\n\r\n\r\n16.61542\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n12.00\r\n\r\n\r\n16.89602\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n11.87\r\n\r\n\r\n16.82587\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n11.33\r\n\r\n\r\n16.49516\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n12.45\r\n\r\n\r\n17.13653\r\n\r\n\r\n   \r\nChecking whether we have got NAs or not:\r\n\r\n\r\nsum(is.na(fustes_6$DBH))\r\n\r\n\r\n[1] 134\r\n\r\n   \r\n  As we can see there are 134 NAs from DBH values that probably represent dead trees, that can be replanted in the future for next rotation. But just to be sure about it, let’s take a look on the age! Because those NAs values also could be trees too much young for DBH.\r\n\r\n\r\nmin(fustes_6$age)\r\n\r\n\r\n[1] 2.811773\r\n\r\n   \r\n  The minimum age turns out the idea of NAs are representing dead trees. Therefore if we would like to get a second rotation forestry would be good to understand the behavior of trees under similar conditions to do future economic analysis. So what would we expect from these trees if they were alive?  \r\n  We can make an approximation through least squares regression model based on our data and variables to find out their diameter at breast height! And for that we need to pre-process the data…    \r\nData pre-processing\r\n   \r\nChecking whether we have got outliers on DBH or not:\r\n\r\n\r\n\r\n  As we can see, there are outliers on the data that we want to remove because it’s not good for least squares regression model. The result bellow show us the lower extreme, first quartile, median, third quartile and the upper extreme respectively.\r\n\r\n\r\nboxplot.stats(fustes_6$DBH)$stats\r\n\r\n\r\n[1]  8.820 11.585 12.510 13.430 16.180\r\n\r\n   \r\nAutomating the process to get the extreme values:\r\n\r\n\r\nupper_whisker <- boxplot.stats(fustes_6$DBH)$stats[5]\r\nlower_whisker <- boxplot.stats(fustes_6$DBH)$stats[1]\r\n\r\n\r\n\r\n   \r\nOf course after the upper extreme we’ve got the outliers and before the first whisker as well. And both we are going to remove this way:\r\n\r\n\r\noutlier_filter_upper <- fustes_6$DBH < upper_whisker \r\noutlier_filter_lower <- fustes_6$DBH > lower_whisker \r\n\r\n#filtering outliers: \r\noutlier_filter <- fustes_6[outlier_filter_lower & outlier_filter_upper,]\r\n\r\n\r\n\r\n   \r\nNow we need to check the class from the variables and if necessary make some changes…\r\n\r\n\r\nstr(fustes_6)\r\n\r\n\r\n'data.frame':   3313 obs. of  6 variables:\r\n $ area(ha)        : num  42.4 42.4 42.4 42.4 42.4 ...\r\n $ spacing         : chr  \"3,45X2,40\" \"3,45X2,40\" \"3,45X2,40\" \"3,45X2,40\" ...\r\n $ genetic material: int  35 35 35 35 35 35 35 35 35 35 ...\r\n $ age             : num  2.84 2.84 2.84 2.84 2.84 ...\r\n $ DBH             : num  12.2 11.5 12 11.9 11.3 ...\r\n $ heigth          : num  17 16.6 16.9 16.8 16.5 ...\r\n\r\n   \r\nCategorical casting variables that we will use to predict:\r\n\r\n\r\nfustes_6$`genetic material` <- as.factor(fustes_6$`genetic material`)\r\n\r\n\r\n\r\n   \r\nBuilding the model\r\n   \r\nOur formula has the following format: \\(\\hat{y}=\\hat{\\beta_0}+\\hat{\\beta_1}X_1+\\hat{\\beta_2}X_2\\)\r\n\r\n\r\ndbh_equation <- \"DBH ~ `genetic material` + age\"\r\n\r\n\r\n\r\n   \r\nPlotting these variables:\r\n\r\n\r\n\r\n   \r\nRegression model:\r\n\r\n\r\n#removing NAs to make the model:\r\nfustes_model <- na.omit(fustes_6)\r\n\r\ndbh_model <- lm(\r\n  formula = dbh_equation, \r\n  data = fustes_model[outlier_filter_lower & outlier_filter_upper,]\r\n)\r\n\r\nsummary(dbh_model)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = dbh_equation, data = fustes_model[outlier_filter_lower & \r\n    outlier_filter_upper, ])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-7.6626 -0.7964  0.1341  1.0160  6.8762 \r\n\r\nCoefficients:\r\n                     Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)           3.87366    2.40738   1.609 0.107708    \r\n`genetic material`12 -0.52911    0.11485  -4.607 4.26e-06 ***\r\n`genetic material`19  0.12014    0.17866   0.672 0.501359    \r\n`genetic material`35 -0.20092    0.08401  -2.392 0.016838 *  \r\n`genetic material`60 -0.26753    0.09896  -2.703 0.006905 ** \r\nage                   3.01675    0.82678   3.649 0.000268 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.619 on 2912 degrees of freedom\r\n  (264 observations deleted due to missingness)\r\nMultiple R-squared:  0.02809,   Adjusted R-squared:  0.02643 \r\nF-statistic: 16.84 on 5 and 2912 DF,  p-value: < 2.2e-16\r\n\r\n   \r\nTherefore, as we could see above the age and genetic material are going to be useful variables on the predictions of DBH through this model!\r\n   \r\nNow we are going to isolate the NA values (that we want to predict) and all the columns that we are using to get DBH values:\r\n\r\n\r\ndbh_rows <- fustes_6[is.na(fustes_6$DBH), c(\"genetic material\",\"age\")]\r\n\r\n\r\n\r\n   \r\nThe last step is to predict and replace those missing values:\r\n\r\n\r\ndbh_predictions <- predict(dbh_model, newdata = dbh_rows)\r\ndbh_predictions\r\n\r\n\r\n      22       80      122      163      175      181      187 \r\n12.23775 12.17994 11.93434 11.93434 11.93434 12.72776 12.72776 \r\n     209      219      324      328      357      378      421 \r\n12.72776 12.72776 11.87653 11.87653 12.08301 12.08301 12.78557 \r\n     448      455      564      576      588      601      602 \r\n12.78557 12.78557 12.60387 12.55431 12.55431 12.55431 12.55431 \r\n     608      627      628      686      694      695      718 \r\n12.55431 12.44424 12.44424 12.62038 12.62038 12.62038 12.62038 \r\n     776      793      831      859      866      869      879 \r\n12.19592 12.16342 12.16342 12.33633 12.33633 12.33633 12.33633 \r\n     886      887      889      891      905      962      990 \r\n12.33633 12.33633 12.33633 12.33633 12.39415 12.61663 12.61663 \r\n    1009     1027     1036     1045     1057     1060     1124 \r\n12.54605 12.54605 12.54605 12.54605 12.54605 12.54605 12.19646 \r\n    1211     1212     1281     1291     1292     1293     1352 \r\n12.56257 12.56257 12.32861 12.79383 12.79383 12.79383 12.42772 \r\n    1427     1471     1475     1479     1480     1481     1490 \r\n12.76079 12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 \r\n    1491     1496     1498     1501     1503     1504     1509 \r\n12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 \r\n    1563     1624     1626     1627     1628     1629     1630 \r\n12.61769 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \r\n    1631     1636     1642     1643     1646     1647     1648 \r\n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \r\n    1649     1650     1651     1660     1662     1663     1664 \r\n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \r\n    1665     1668     1669     1670     1671     1676     1677 \r\n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \r\n    1678     1679     1711     1748     1777     1781     1782 \r\n12.79383 12.79383 11.84349 11.85175 11.85175 11.85175 11.85175 \r\n    1788     1791     1794     1873     1886     1923     1924 \r\n11.85175 11.85175 11.85175 12.21297 12.21297 12.22123 12.22123 \r\n    1942     1943     1967     1973     1982     2003     2006 \r\n12.22123 12.22123 12.22123 12.73602 12.73602 12.73602 12.73602 \r\n    2007     2042     2043     2090     2095     2139     2145 \r\n12.73602 12.60387 12.60387 12.22070 12.22070 12.71950 12.71950 \r\n    2200     2201     2208     2226     2400     2626     2799 \r\n12.18820 12.18820 12.18820 12.18820 12.55431 12.25427 12.71124 \r\n    2826 \r\n12.71124 \r\n\r\nFilling the data with predicted values:\r\n\r\n\r\nfustes_6[is.na(fustes_6$DBH), \"DBH\"] <- dbh_predictions\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-18-predicting-na-values/predicting-na-values_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-03-04T14:59:42-03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-18-visualizing-maps-interactively/",
    "title": "Visualizing Maps Interactively",
    "description": "My first shiny app! \nUsing the results from ensemble models of my scientific initiation\nto define the priority areas for conservation of Brazil nut",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-02-18",
    "categories": [],
    "contents": "\r\n \r\n \r\n\r\nVisualizing maps in a simple way\r\n\r\n \r\n \r\nIntroduction\r\n   \r\n  First of all, what made me get started and learn shiny was the facility provided by the app for visualization. This is quite useful when you’ve got an amount of data to visualize and compare. Of course any GIS software is enough to do this job! But it’s not easy to handle and share a set of maps keeping the interactivity from them so that’s why I was searching how to make it properly. And everyone that I know would rather enjoy beautiful interactive graphics or maps to analyse the data. I think this is the best way to deal with the results, it is always good to simplify the visualization before studying it further theoretically.  \r\n  Well, the maps analysed were results from a prediction made by ensemble models to find out the priority areas for conservation of Bertholletia excelsa using different filters (environmental and geographical) based on 15 variables defined as the best in predicting the suitability of Brazil nut by Daiana tourne et. al 2019 on her doctorate degree using principal component analyses (PCA) combined with expert analyses.  \r\n  Therefore the main objective is directly correlated with the climatic analyses and soon we will combine with genetic analyses. These results presented here are considering the actual conditions using the variables from different databases as Worldclim, USGS, etc. More details about the research will be posted in a formal article in the future when it gets ready. Is important to say that in this research I have got 3 co-authors from Bioversity International which are: Evert Thomas, Tobias Fremout and Viviana Ceccarelli. Besides them Daiana also is helping in this project as well as my advisor Karina Martins.  \r\n  The shiny app has a good and organized structure quite easy to understand which helped me a lot to generate the app. However I didn’t use packages as Golem to provide a code cleaner so it was a robust script which I have written without using modules, because although the app has a clear structure and easy to apply modules I wasn’t familiar with it.\r\n \r\n \r\nExploring the maps\r\n   \r\n  The app has 6 tabs with different filters and content as you can see below:\r\nSuitability - Environmental filtering\r\nCount - Environmental filtering\r\nBinary - Environmental filtering\r\nSuitability - Geographical filtering\r\nCount - Geographical filtering\r\nBinary - Geographical filtering\r\n\r\n\r\n  So as I said before the model is an ensemble of different algorithms (MAXENT, RF, GLMNET, etc). Each of these algorithms produces a different presence map. The suitability map shows us the probability (0-1000) to get the presence of the species occurring at the site assuming a specific threshold based on the model filter which could be Environmental or Geographical. The count map indicates how many algorithms predict suitable a point, from 0 algorithms to 8 algorithms which is the maximum number of algorithms which predict suitable the same grid from 10 algorithms used in the ensemble initially. And last but not least the binary map shows us absence and presence (0-1) also considering the same threshold for suitability map.    \r\nI will leave here the Shiny app if you would like to take a look further.\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/Gabrielforest/portfolio/blob/main/previewblog.PNG?raw=true",
    "last_modified": "2023-03-04T14:59:42-03:00",
    "input_file": {}
  }
]
