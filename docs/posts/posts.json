[
  {
    "path": "posts/2021-07-24-gas-emission-dashboard/",
    "title": "BR - Gas Emission Dashboard",
    "description": "My first dashboard on Power BI! \nUsing a query of gas emission dataset from (https://basedosdados.org/) to visualize the situation of several states for 2018-2019",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-07-25",
    "categories": [],
    "contents": "\r\n \r\n \r\n\r\nQuerying and visualizing data\r\n\r\n \r\n \r\nIntroduction\r\n \r\n  Currently I am on vacation from university so I’ve started my studies on SQL through SQL Server desktop. After the beginning learning process, when I learned the structure to create databases and became myself able to make some queries, I decided to make a project with real data instead just working with simple databases created by my own. Besides that, I tried to gather both beginner knowledges of SQL and Power BI to use the data to make a dashboard. Although I am new using these tools, this is not my first time doing a dashboard, I’ve made it through R language and you can check an example of it through this repository which also includes a github page as I am used to do on my github.\r\n \r\n  After this period learning the basics (which was quite fast because the “SEQUEL” is very similar with English and the package dplyr from R, which probably is based on SQL) I’ve started to practice it using the Google BigQuery which is a fully-managed, serverless data warehouse that enables scalable analysis over petabytes of data. And I would like to comment that It was very impressive for me how it works so fast using an amount of data that my computer would never support…\r\n \r\n  Anyways I have been using it a lot in order to practice SQL and generate quickly insights about different interesting subjects through Data Studio (which I am studying as well) within the same platform. The basedosdados initiative to gather the public information from Brazil in a well organized way is very useful to combine several datasets through BigQuery and produce valuable and palpable perspectives about the content produced around us. That’s why I chose a dataset available there, to get my first dashboard on Power BI, possibly divulging this organization for mates that are reading this.\r\n \r\n  I have written this following query to get my data which is totally reproducible thanks to basedosdados:\r\n-- gas emission of CO2e (t) GWP-AR5 \r\n-- table joined with population data from IBGE by state for 2018 and 2019\r\n-- considering 4 different sectors.\r\n\r\nWITH \r\nt1 AS(\r\nSELECT \r\n  ano,\r\n  nivel_1 AS setor,\r\n  sigla_uf AS estado,\r\n  gas,\r\n  SUM(emissao) AS emissao\r\nFROM \r\n  `basedosdados.br_seeg_emissoes.uf` \r\nWHERE \r\n  ano = 2018 OR ano = 2019\r\n  AND gas = 'CO2e (t) GWP-AR5'\r\nGROUP BY\r\n  setor, estado, ano, gas\r\n),\r\nt2 AS(\r\nSELECT \r\n  ano,\r\n  nivel_1 AS setor,\r\n  sigla_uf AS estado\r\nFROM\r\n  `basedosdados.br_seeg_emissoes.uf`\r\nWHERE\r\n  ano = 2018 OR ano = 2019\r\n  AND gas = 'CO2e (t) GWP-AR5'\r\nGROUP BY\r\n  ano, setor, estado\r\n),\r\nt3 AS(\r\nSELECT \r\n  ano, \r\n  sigla_uf, \r\n  populacao\r\nFROM\r\n  `basedosdados.br_ibge_populacao.uf`\r\nWHERE\r\n  ano = 2018 OR ano = 2019\r\nGROUP BY \r\n  sigla_uf, ano, populacao\r\n),\r\nt4 AS(\r\nSELECT \r\n  t2.ano,\r\n  t2.setor,\r\n  t2.estado,\r\n  t3.populacao\r\nFROM \r\n  t2\r\nJOIN\r\n  t3 \r\n  ON t2.ano = t3.ano\r\n  AND t2.estado = t3.sigla_uf \r\n)\r\nSELECT\r\n  t4.ano,\r\n  t4.setor,\r\n  t4.estado,\r\n  t4.populacao,\r\n  t1.gas,\r\n  t1.emissao\r\nFROM \r\n  t4\r\nJOIN\r\n  t1\r\n  ON t4.ano = t1.ano\r\n  AND t4.setor = t1.setor\r\n  AND t4.estado = t1.estado\r\nWHERE \r\n  gas = 'CO2e (t) GWP-AR5'\r\n \r\n \r\nQueried Data\r\n   \r\n  YEARS: the years covered on the query were 2018 and 2019.  \r\n  POPULATION: population of 2018-2019.  \r\n  STATES: all the states from Brazil.  \r\n  SECTORS: the data has 5 sectors and each one cover a determined area as specified below  \r\nagriculture and livestock\r\nrice cultivation\r\nenteric fermentation\r\nanimal waste management\r\nburning agricultural waste\r\nmanaged soils\r\nenergy\r\nfugitive emissions\r\nemissions from fuel combustion\r\nwaste\r\nliquid effluents\r\nsolid waste\r\nindustrial processes\r\nHFCs emissions\r\nchemical industry\r\nminearl products\r\nmetal production\r\nnon-energy use of fuels and use of solvents\r\nuse of SF6\r\nland and forest use change\r\nland use changes\r\nforest residues\r\n    EMISSION: the gas that I’ve selected to compose this field is the equivalent carbon (t) GWP-AR5. Is important to say that there are two main approaches to determining the equivalent carbon: GWP (Global Warming Potential) and GTP (Global Temperature Change Potential). The first one considers the influence of the gases in the alteration of the energy balance of the Earth and, second, the influence in the increase of temperature. Both are measured for a term of 100 years, with GWP being most commonly used and the ‘AR5’ refers to the total emissions of greenhouse gases which were computed in the fith inventory defined by the Intergovernmental Panel on Climate Change (IPCC) guidelines for national inventories, that’s why I chose this one. You can check more information about the data at this link.\r\n     \r\nDashboard\r\n   \r\n\r\n\r\nConclusion\r\n   \r\nI really enjoy the process to make this project learning SQL and Power Bi together which definitely allows me to spread my analyses combining several tools and keep learning it at the same time.\r\nTo finish, I would like to share the DAX formula that I’ve used to get the population by year and state on my dashboard:\r\n   \r\npop 2019 = \r\n\r\nCALCULATE ( \r\n  \r\n  SUMX ( \r\n  \r\n    VALUES ( 'sql_gas_emission_2 (2)'[estado] ),\r\n    \r\n    CALCULATE( \r\n      MAX ( 'sql_gas_emission_2 (2)'[populacao] ) \r\n        ) \r\n  \r\n  ), \r\n  \r\n'sql_gas_emission_2 (2)'[ano] = 2019\r\n) \r\n\r\n\r\n\r\n",
    "preview": "https://github.com/Gabrielforest/portfolio/blob/main/dash_img.PNG?raw=true",
    "last_modified": "2021-07-25T21:36:13-03:00",
    "input_file": "gas-emission-dashboard.knit.md"
  },
  {
    "path": "posts/2021-04-18-predicting-na-values/",
    "title": "Predicting Missing Values",
    "description": "Analising forest inventory data and predicting NAs through least squares regression model",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-04-18",
    "categories": [],
    "contents": "\r\nIntroduction\r\n   \r\n  This data was provided by my dendrometry teacher and it is the result of a sistematic collection of data that was carried out between 2017 and 2018 at Três Lagoas, which is a city from Mato Grosso do Sul state. The area has an extent of 1226 acres of Eucalyptus grandis W. Hill ex Maiden x Eucalyptus urophylla S. T. Blake clonal culture with mean age of 2.88 years.\r\n\r\n\r\n\r\nExploring the data\r\n   \r\n  Here we can take a look on the first 6 rows from the data frame which has 3313 obs. of 6 variables:\r\n\r\n\r\narea(ha)\r\n\r\n\r\nspacing\r\n\r\n\r\ngenetic material\r\n\r\n\r\nage\r\n\r\n\r\nDBH\r\n\r\n\r\nheigth\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n12.16\r\n\r\n\r\n16.98621\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n11.52\r\n\r\n\r\n16.61542\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n12.00\r\n\r\n\r\n16.89602\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n11.87\r\n\r\n\r\n16.82587\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n11.33\r\n\r\n\r\n16.49516\r\n\r\n\r\n42.43\r\n\r\n\r\n3,45X2,40\r\n\r\n\r\n35\r\n\r\n\r\n2.839151\r\n\r\n\r\n12.45\r\n\r\n\r\n17.13653\r\n\r\n\r\n   \r\nChecking whether we have got NAs or not:\r\n\r\n\r\nsum(is.na(fustes_6$DBH))\r\n\r\n\r\n[1] 134\r\n\r\n   \r\n  As we can see there are 134 NAs from DBH values that probably represent dead trees, that can be replanted in the future for next rotation. But just to be sure about it, let’s take a look on the age! Because those NAs values also could be trees too much young for DBH.\r\n\r\n\r\nmin(fustes_6$age)\r\n\r\n\r\n[1] 2.811773\r\n\r\n   \r\n  The minimum age turns out the idea of NAs are representing dead trees. Therefore if we would like to get a second rotation forestry would be good to understand the behavior of trees under similar conditions to do future economic analysis. So what would we expect from these trees if they were alive?  \r\n  We can make an approximation through least squares regression model based on our data and variables to find out their diameter at breast height! And for that we need to pre-process the data…    \r\nData pre-processing\r\n   \r\nChecking whether we have got outliers on DBH or not:\r\n\r\n\r\n\r\n  As we can see, there are outliers on the data that we want to remove because it’s not good for least squares regression model. The result bellow show us the lower extreme, first quartile, median, third quartile and the upper extreme respectively.\r\n\r\n\r\nboxplot.stats(fustes_6$DBH)$stats\r\n\r\n\r\n[1]  8.820 11.585 12.510 13.430 16.180\r\n\r\n   \r\nAutomating the process to get the extreme values:\r\n\r\n\r\nupper_whisker <- boxplot.stats(fustes_6$DBH)$stats[5]\r\nlower_whisker <- boxplot.stats(fustes_6$DBH)$stats[1]\r\n\r\n\r\n\r\n   \r\nOf course after the upper extreme we’ve got the outliers and before the first whisker as well. And both we are going to remove this way:\r\n\r\n\r\noutlier_filter_upper <- fustes_6$DBH < upper_whisker \r\noutlier_filter_lower <- fustes_6$DBH > lower_whisker \r\n\r\n#filtering outliers: \r\noutlier_filter <- fustes_6[outlier_filter_lower & outlier_filter_upper,]\r\n\r\n\r\n\r\n   \r\nNow we need to check the class from the variables and if necessary make some changes…\r\n\r\n\r\nstr(fustes_6)\r\n\r\n\r\n'data.frame':   3313 obs. of  6 variables:\r\n $ area(ha)        : num  42.4 42.4 42.4 42.4 42.4 ...\r\n $ spacing         : chr  \"3,45X2,40\" \"3,45X2,40\" \"3,45X2,40\" \"3,45X2,40\" ...\r\n $ genetic material: int  35 35 35 35 35 35 35 35 35 35 ...\r\n $ age             : num  2.84 2.84 2.84 2.84 2.84 ...\r\n $ DBH             : num  12.2 11.5 12 11.9 11.3 ...\r\n $ heigth          : num  17 16.6 16.9 16.8 16.5 ...\r\n\r\n   \r\nCategorical casting variables that we will use to predict:\r\n\r\n\r\nfustes_6$`genetic material` <- as.factor(fustes_6$`genetic material`)\r\n\r\n\r\n\r\n   \r\nBuilding the model\r\n   \r\nOur formula has the following format: \\(\\hat{y}=\\hat{\\beta_0}+\\hat{\\beta_1}X_1+\\hat{\\beta_2}X_2\\)\r\n\r\n\r\ndbh_equation <- \"DBH ~ `genetic material` + age\"\r\n\r\n\r\n\r\n   \r\nPlotting these variables:\r\n\r\n\r\n\r\n   \r\nRegression model:\r\n\r\n\r\n#removing NAs to make the model:\r\nfustes_model <- na.omit(fustes_6)\r\n\r\ndbh_model <- lm(\r\n  formula = dbh_equation, \r\n  data = fustes_model[outlier_filter_lower & outlier_filter_upper,]\r\n)\r\n\r\nsummary(dbh_model)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = dbh_equation, data = fustes_model[outlier_filter_lower & \r\n    outlier_filter_upper, ])\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-7.6626 -0.7964  0.1341  1.0160  6.8762 \r\n\r\nCoefficients:\r\n                     Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)           3.87366    2.40738   1.609 0.107708    \r\n`genetic material`12 -0.52911    0.11485  -4.607 4.26e-06 ***\r\n`genetic material`19  0.12014    0.17866   0.672 0.501359    \r\n`genetic material`35 -0.20092    0.08401  -2.392 0.016838 *  \r\n`genetic material`60 -0.26753    0.09896  -2.703 0.006905 ** \r\nage                   3.01675    0.82678   3.649 0.000268 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1.619 on 2912 degrees of freedom\r\n  (264 observations deleted due to missingness)\r\nMultiple R-squared:  0.02809,   Adjusted R-squared:  0.02643 \r\nF-statistic: 16.84 on 5 and 2912 DF,  p-value: < 2.2e-16\r\n\r\n   \r\nTherefore, as we could see above the age and genetic material are going to be useful variables on the predictions of DBH through this model!\r\n   \r\nNow we are going to isolate the NA values (that we want to predict) and all the columns that we are using to get DBH values:\r\n\r\n\r\ndbh_rows <- fustes_6[is.na(fustes_6$DBH), c(\"genetic material\",\"age\")]\r\n\r\n\r\n\r\n   \r\nThe last step is to predict and replace those missing values:\r\n\r\n\r\ndbh_predictions <- predict(dbh_model, newdata = dbh_rows)\r\ndbh_predictions\r\n\r\n\r\n      22       80      122      163      175      181      187 \r\n12.23775 12.17994 11.93434 11.93434 11.93434 12.72776 12.72776 \r\n     209      219      324      328      357      378      421 \r\n12.72776 12.72776 11.87653 11.87653 12.08301 12.08301 12.78557 \r\n     448      455      564      576      588      601      602 \r\n12.78557 12.78557 12.60387 12.55431 12.55431 12.55431 12.55431 \r\n     608      627      628      686      694      695      718 \r\n12.55431 12.44424 12.44424 12.62038 12.62038 12.62038 12.62038 \r\n     776      793      831      859      866      869      879 \r\n12.19592 12.16342 12.16342 12.33633 12.33633 12.33633 12.33633 \r\n     886      887      889      891      905      962      990 \r\n12.33633 12.33633 12.33633 12.33633 12.39415 12.61663 12.61663 \r\n    1009     1027     1036     1045     1057     1060     1124 \r\n12.54605 12.54605 12.54605 12.54605 12.54605 12.54605 12.19646 \r\n    1211     1212     1281     1291     1292     1293     1352 \r\n12.56257 12.56257 12.32861 12.79383 12.79383 12.79383 12.42772 \r\n    1427     1471     1475     1479     1480     1481     1490 \r\n12.76079 12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 \r\n    1491     1496     1498     1501     1503     1504     1509 \r\n12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 12.71950 \r\n    1563     1624     1626     1627     1628     1629     1630 \r\n12.61769 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \r\n    1631     1636     1642     1643     1646     1647     1648 \r\n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \r\n    1649     1650     1651     1660     1662     1663     1664 \r\n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \r\n    1665     1668     1669     1670     1671     1676     1677 \r\n12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 12.79383 \r\n    1678     1679     1711     1748     1777     1781     1782 \r\n12.79383 12.79383 11.84349 11.85175 11.85175 11.85175 11.85175 \r\n    1788     1791     1794     1873     1886     1923     1924 \r\n11.85175 11.85175 11.85175 12.21297 12.21297 12.22123 12.22123 \r\n    1942     1943     1967     1973     1982     2003     2006 \r\n12.22123 12.22123 12.22123 12.73602 12.73602 12.73602 12.73602 \r\n    2007     2042     2043     2090     2095     2139     2145 \r\n12.73602 12.60387 12.60387 12.22070 12.22070 12.71950 12.71950 \r\n    2200     2201     2208     2226     2400     2626     2799 \r\n12.18820 12.18820 12.18820 12.18820 12.55431 12.25427 12.71124 \r\n    2826 \r\n12.71124 \r\n\r\nFilling the data with predicted values:\r\n\r\n\r\nfustes_6[is.na(fustes_6$DBH), \"DBH\"] <- dbh_predictions\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-18-predicting-na-values/predicting-na-values_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-07-24T19:27:00-03:00",
    "input_file": "predicting-na-values.knit.md"
  },
  {
    "path": "posts/2021-02-18-visualizing-maps-interactively/",
    "title": "Visualizing Maps Interactively",
    "description": "My first shiny app! \nUsing the results from ensemble models of my scientific initiation\nto define the priority areas for conservation of Brazil nut",
    "author": [
      {
        "name": "Gabriel de Freitas Pereira",
        "url": {}
      }
    ],
    "date": "2021-02-18",
    "categories": [],
    "contents": "\r\n \r\n \r\n\r\nVisualizing maps in a simple way\r\n\r\n \r\n \r\nIntroduction\r\n   \r\n  First of all, what made me get started and learn shiny was the facility provided by the app for visualization. This is quite useful when you’ve got an amount of data to visualize and compare. Of course any GIS software is enough to do this job! But it’s not easy to handle and share a set of maps keeping the interactivity from them so that’s why I was searching how to make it properly. And everyone that I know would rather enjoy beautiful interactive graphics or maps to analyse the data. I think this is the best way to deal with the results, it is always good to simplify the visualization before studying it further theoretically.  \r\n  Well, the maps analysed were results from a prediction made by ensemble models to find out the priority areas for conservation of Bertholletia excelsa using different filters (environmental and geographical) based on 15 variables defined as the best in predicting the suitability of Brazil nut by Daiana tourne et. al 2019 on her doctorate degree using principal component analyses (PCA) combined with expert analyses.  \r\n  Therefore the main objective is directly correlated with the climatic analyses and soon we will combine with genetic analyses. These results presented here are considering the actual conditions using the variables from different databases as Worldclim, USGS, etc. More details about the research will be posted in a formal article in the future when it gets ready. Is important to say that in this research I have got 3 co-authors from Bioversity International which are: Evert Thomas, Tobias Fremout and Viviana Ceccarelli. Besides them Daiana also is helping in this project as well as my advisor Karina Martins.  \r\n  The shiny app has a good and organized structure quite easy to understand which helped me a lot to generate the app. However I didn’t use packages as Golem to provide a code cleaner so it was a robust script which I have written without using modules, because although the app has a clear structure and easy to apply modules I wasn’t familiar with it.\r\n \r\n \r\nExploring the maps\r\n   \r\n  The app has 6 tabs with different filters and content as you can see below:\r\nSuitability - Environmental filtering\r\nCount - Environmental filtering\r\nBinary - Environmental filtering\r\nSuitability - Geographical filtering\r\nCount - Geographical filtering\r\nBinary - Geographical filtering\r\n\r\n\r\n  So as I said before the model is an ensemble of different algorithms (MAXENT, RF, GLMNET, etc). Each of these algorithms produces a different presence map. The suitability map shows us the probability (0-1000) to get the presence of the species occurring at the site assuming a specific threshold based on the model filter which could be Environmental or Geographical. The count map indicates how many algorithms predict suitable a point, from 0 algorithms to 8 algorithms which is the maximum number of algorithms which predict suitable the same grid from 10 algorithms used in the ensemble initially. And last but not least the binary map shows us absence and presence (0-1) also considering the same threshold for suitability map.    \r\nI will leave here the Shiny app if you would like to take a look further.\r\n\r\n\r\n\r\n",
    "preview": "https://github.com/Gabrielforest/portfolio/blob/main/previewblog.PNG?raw=true",
    "last_modified": "2021-03-21T20:08:41-03:00",
    "input_file": {}
  }
]
